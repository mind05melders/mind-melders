{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "p4PDM-e5YRFt",
        "outputId": "44caa300-0629-4496-e198-f492d2f522e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved the pretrained model\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='955' max='955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [955/955 05:28, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.239600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved the trained model\n",
            "######### TIME TAKEN FOR TOTAL CODE : 352.7306127548218\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def load_dataset(file_path, tokenizer, block_size = 128):\n",
        "    dataset = TextDataset(\n",
        "        tokenizer = tokenizer,\n",
        "        file_path = file_path,\n",
        "        block_size = block_size,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def load_data_collator(tokenizer, mlm = False):\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=mlm,\n",
        "    )\n",
        "    return data_collator\n",
        "\n",
        "def train(train_file_path,model_name,\n",
        "          output_dir,\n",
        "          overwrite_output_dir,\n",
        "          per_device_train_batch_size,\n",
        "          num_train_epochs,\n",
        "          save_steps,save_total_limit):\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "    tokenizer.add_special_tokens({\n",
        "        \"pad_token\":\"<pad>\",\n",
        "        \"bos_token\":\"<startofstring>\",\n",
        "        \"eos_token\":\"<endofstring>\"\n",
        "    })\n",
        "    tokenizer.add_tokens([\"<bot>:\"])\n",
        "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
        "    data_collator = load_data_collator(tokenizer)\n",
        "\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.to(device)\n",
        "\n",
        "    model.save_pretrained(output_dir)\n",
        "    print(\"Saved the pretrained model\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          overwrite_output_dir=overwrite_output_dir,\n",
        "          per_device_train_batch_size=per_device_train_batch_size,\n",
        "          num_train_epochs=num_train_epochs,\n",
        "          save_total_limit=save_total_limit,\n",
        "#           save_steps=save_steps\n",
        "      )\n",
        "\n",
        "    trainer = Trainer(\n",
        "          model=model,\n",
        "          args=training_args,\n",
        "          data_collator=data_collator,\n",
        "          train_dataset=train_dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model()\n",
        "    print(\"Saved the trained model\")\n",
        "\n",
        "\n",
        "train_file_path = \"/content/output.txt\"\n",
        "model_name = 'gpt2-medium'\n",
        "output_dir = '/content/drive/MyDrive/chat_models'\n",
        "overwrite_output_dir = True\n",
        "per_device_train_batch_size = 2\n",
        "num_train_epochs = 5\n",
        "save_steps = 500\n",
        "save_total_limit=2\n",
        "\n",
        "\n",
        "total_start = time.time()\n",
        "\n",
        "# Train\n",
        "train(\n",
        "    train_file_path=train_file_path,\n",
        "    model_name=model_name,\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=overwrite_output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    save_steps=save_steps,\n",
        "    save_total_limit=save_total_limit\n",
        ")\n",
        "\n",
        "print(\"######### TIME TAKEN FOR TOTAL CODE : {}\".format(time.time() - total_start))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv7hk6o6aGyO",
        "outputId": "6cd7c93a-474b-4f8a-fef3-cedfe71ec28f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Enter a prompt (or 'exit' to quit): begin WriteLn('Don't worry, everything will be fine.') end.\n",
            "\n",
            "Generated Text:\n",
            "begin WriteLn('Don't worry, everything will be fine.') end.  :     :  print('Don't worry, everything will be fine.') \n",
            "Enter a prompt (or 'exit' to quit): uses SysUtils; procedure Swap(var a, b: Integer); var temp: Integer; begin temp := a; a := b; b := temp; end; var x, y: Integer; begin x := 10; y := 20; WriteLn('Before swapping:'); WriteLn('x = ', x); WriteLn('y = ', y); Swap(x, y); WriteLn('After swapping:'); WriteLn('x = ', x); WriteLn('y = ', y); ReadLn; end.\n",
            "\n",
            "Generated Text:\n",
            "uses SysUtils; procedure Swap(var a, b:   Integer); var temp: Integer; begin temp := a; a := b; b := temp; end; var x, y: Integer; begin x := 10; y := 20; WriteLn('Before swapping:'); WriteLn('x = ', x); WriteLn('y = ', y); Swap(x, y); WriteLn('After swapping:'); WriteLn('x = ', x); WriteLn('y = ', y); ReadLn; end.  :   :  def swap(r, k): return k, r x = 10 y = 20 print(\"Before swapping:\") print(\"x =\", x) print(\"y =\", y) x, y = swap(x, y) print(\"After swapping:\") print(\"x =\", x) print(\"y =\", y) \n",
            "Enter a prompt (or 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/drive/MyDrive/chat_models')\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/chat_models')\n",
        "\n",
        "# Set the device to use (e.g., 'cuda' for GPU or 'cpu' for CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "print(device)\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=500):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "\n",
        "    # Set pad_token_id to eos_token_id for open-end generation\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    output = model.generate(input_ids, max_length=max_length, attention_mask=attention_mask, pad_token_id=pad_token_id)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Find the index of the first occurrence of ':' (colon)\n",
        "    colon_index = generated_text.find(':')\n",
        "    if colon_index != -1:\n",
        "        # Check if there is another colon after the first one\n",
        "        next_colon_index = generated_text.find(':', colon_index + 1)\n",
        "        if next_colon_index != -1:\n",
        "            # Insert '<bot>' only after the first colon\n",
        "            generated_text = generated_text[:colon_index + 1] + ' <bot> ' + generated_text[colon_index + 1:]\n",
        "\n",
        "    # Replace all other occurrences of '<bot>' with an empty string\n",
        "    generated_text = generated_text.replace('<bot>', '')\n",
        "\n",
        "    return generated_text.strip()  # Strip any leading or trailing whitespace\n",
        "# Prompt for user input and generate text\n",
        "while True:\n",
        "    user_prompt = input(\"Enter a prompt (or 'exit' to quit): \")\n",
        "    if user_prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    exit_value = user_prompt\n",
        "    user_prompt = \"<startofstring> \" + user_prompt + \" <bot>: \"\n",
        "\n",
        "    # Generate text\n",
        "    generated_output = generate_text(user_prompt)\n",
        "\n",
        "    # Print only the generated output\n",
        "    print(\"\\nGenerated Text:\")\n",
        "    print(generated_output.split('\\n')[0])  # Print only the first line of generated output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUpGGvKEbyMr",
        "outputId": "4ffcccd7-a9c9-4059-b5ce-3a1a9a444251"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "\n",
            "Generated Text:\n",
            "Convert the following Pascal code to Python code:\n",
            "Pascal:\n",
            "program DataOperationWithDatabase; {$APPTYPE CONSOLE} uses System.SysUtils, System.Classes, Data.DB, Data.SqlExpr, Data.DBXCommon, System.Generics.Collections; type TDataOperation = class public class function PerformOperation(const Input: Integer): Integer; virtual; abstract; end; TAdditionOperation = class(TDataOperation) public class function PerformOperation(const Input: Integer): Integer; override; end; TSubtractionOperation = class(TDataOperation) public class function PerformOperation(const Input: Integer): Integer; override; end; TDatabaseUpdater = class public class procedure UpdateDatabase(OperationName: string; Input, Output: Integer); end; { TAdditionOperation } class function TAdditionOperation.PerformOperation(const Input: Integer): Integer; begin Result := Input + 10; end; { TSubtractionOperation } class function TSubtractionOperation.PerformOperation(const Input: Integer): Integer; begin Result := Input - 5; end; { TDatabaseUpdater } class procedure TDatabaseUpdater.UpdateDatabase(OperationName: string; Input, Output: Integer); begin Writeln('Updating database with operation:', OperationName, ', Input:', Input, ', Output:', Output); end; var InputNumber: Integer; OperationType: Char; OutputNumber: Integer; Operation: TDataOperation; begin try Writeln('Welcome to Data Operation Program'); Write('Enter operation type (A for Addition, S for Subtraction): '); Readln(OperationType); Write('Enter a number: '); Readln(InputNumber); case OperationType of 'A': Operation := TAdditionOperation.Create; 'S': Operation := TSubtractionOperation.Create; else begin Writeln('Invalid operation type.'); Exit; end; end; OutputNumber := Operation.PerformOperation(InputNumber); Writeln('Result:', OutputNumber); TDatabaseUpdater.UpdateDatabase(OperationType, InputNumber, OutputNumber); Operation.Free; except on E: Exception do Writeln('Error:', E.ClassName, ':', E.Message); end; end.\n",
            "Python:\n",
            "  <bot>:   <bot>:  import sys, random, class DataOperation: def PerformOperation(input_value): return input_value + 10 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value - 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value * 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value / 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class DataOperation(TDataOperation): @staticmethod def PerformOperation(input_value): return input_value % 5 class Data\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('/content/drive/MyDrive/chat_models')\n",
        "model = GPT2LMHeadModel.from_pretrained('/content/drive/MyDrive/chat_models')\n",
        "\n",
        "tokenizer.add_special_tokens({\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"bos_token\": \"<startofstring>\",\n",
        "    \"eos_token\": \"<endofstring>\"\n",
        "})\n",
        "\n",
        "tokenizer.add_tokens([\"<bot>:\"])\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set the device to use (e.g., 'cuda' for GPU or 'cpu' for CPU)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "print(device)\n",
        "\n",
        "# Function to split input text into chunks\n",
        "def split_into_chunks(text, max_length):\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
        "    return chunks\n",
        "\n",
        "# Function to generate text for a single chunk\n",
        "def generate_text_from_chunk(chunk, max_length=1024):\n",
        "    input_ids = torch.tensor(chunk).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
        "\n",
        "    # Set pad_token_id to eos_token_id for open-end generation\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    output = model.generate(input_ids, max_length=max_length, attention_mask=attention_mask, pad_token_id=pad_token_id)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    return generated_text.strip()  # Strip any leading or trailing whitespace\n",
        "\n",
        "# Function to generate text from the entire input\n",
        "def generate_text(prompt, max_length=1024):\n",
        "    chunks = split_into_chunks(prompt, max_length - 50)  # Leaving space for additional tokens and special tokens\n",
        "    generated_output = \"\"\n",
        "\n",
        "    for chunk in chunks:\n",
        "        chunk_output = generate_text_from_chunk(chunk, max_length)\n",
        "        generated_output += chunk_output + \" \"  # Ensure separation between chunks\n",
        "\n",
        "    return generated_output\n",
        "\n",
        "# Provide clear instructions in the prompt\n",
        "def prepare_prompt(user_prompt):\n",
        "    instruction = (\n",
        "        \"Convert the following Pascal code to Python code:\\n\"\n",
        "        \"Pascal:\\n\"\n",
        "        f\"{user_prompt}\\n\"\n",
        "        \"Python:\\n\"\n",
        "    )\n",
        "    return \"<startofstring> \" + instruction + \" <bot>: \"\n",
        "\n",
        "# Prompt for user input and generate text\n",
        "while True:\n",
        "    user_prompt = input(\"Enter a prompt (or 'exit' to quit): \")\n",
        "    if user_prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    prompt_with_instruction = prepare_prompt(user_prompt)\n",
        "\n",
        "    # Generate text\n",
        "    generated_output = generate_text(prompt_with_instruction)\n",
        "\n",
        "    # Print the generated output\n",
        "    print(\"\\nGenerated Text:\")\n",
        "    print(generated_output.strip())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}